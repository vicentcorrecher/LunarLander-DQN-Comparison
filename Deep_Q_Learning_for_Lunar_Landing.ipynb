{"cells":[{"cell_type":"markdown","metadata":{"id":"LbZcI9ZXHl3a"},"source":["# Deep Q-Learning for Lunar Landing"]},{"cell_type":"markdown","source":["El aterrizaje lunar ha sido un desafío emocionante y emblemático en la historia de la exploración espacial. La capacidad de aterrizar un módulo lunar de manera segura y precisa en la superficie de la luna es fundamental para el éxito de las misiones espaciales. En un intento de abordar este desafío, el campo del aprendizaje por refuerzo ha surgido como un enfoque interesante para abordarlo.\n","\n","En este proyecto, nos adentramos en el mundo del aprendizaje por refuerzo y nos enfrentamos al desafío del aterrizaje lunar utilizando Deep Q-Learning (DQN), una técnica que combina el aprendizaje profundo con métodos de aprendizaje por refuerzo. Nuestro objetivo es entrenar un agente de inteligencia artificial para aterrizar un módulo lunar en el entorno simulado de Lunar Lander de OpenAI Gym.\n","\n","**LUNAR LANDER ENVIRONMENT**\n","\n","Lunar Lander es un entorno clásico de aprendizaje por refuerzo que simula el desafío de aterrizar un módulo lunar en la superficie lunar. El agente controla la inclinación y la potencia del motor del módulo lunar para guiar su descenso y evitar colisiones. El objetivo es lograr un aterrizaje suave y seguro, maximizando al mismo tiempo la eficiencia del combustible.\n","\n","**Enfoque DQN**\n","\n","Utilizamos el algoritmo DQN para entrenar nuestro agente en el entorno Lunar Lander. DQN aprovecha las capacidades de las redes neuronales profundas para aproximar la función de valor Q, que mapea estados y acciones a valores de recompensa esperados. A través del entrenamiento iterativo y la exploración del entorno, nuestro agente aprende gradualmente a tomar decisiones inteligentes y aterrizar de manera efectiva en la luna.\n","\n","Implementación y Resultados\n","En nuestro proyecto, implementamos la arquitectura de la red neuronal, configuramos los hiperparámetros clave y diseñamos una estrategia de entrenamiento efectiva. A lo largo de múltiples episodios de entrenamiento, nuestro agente adquiere experiencia, mejora su rendimiento y, finalmente, alcanza la capacidad de aterrizar de manera consistente y eficiente en la luna.\n","\n","Conclusiones y Perspectivas Futuras\n","El proyecto no solo demuestra la capacidad del enfoque DQN para resolver problemas complejos de aprendizaje por refuerzo, sino que también destaca el potencial de la inteligencia artificial para abordar desafíos del mundo real en la exploración espacial y más allá. Con futuras investigaciones y refinamientos, este enfoque podría tener aplicaciones significativas en la navegación autónoma de naves espaciales y la exploración de otros planetas."],"metadata":{"id":"xbERHt_D-qkH"}},{"cell_type":"markdown","metadata":{"id":"E8yPRjteXgPb"},"source":["### Instalación e importación librerías"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dbnq3XpoKa_7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ebff95e-9e1f-4108-a07e-687f4ffba1f0","executionInfo":{"status":"ok","timestamp":1716619827799,"user_tz":-120,"elapsed":115515,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/953.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.2/953.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/953.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.11.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n","Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n","  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.4)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n","  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2024.2.2)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=25107608e69dd36b087c4561462de92b4570dcaf9ce925c21c0180386191f3e3\n","  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n","Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  swig4.0\n","Suggested packages:\n","  swig-doc swig-examples swig4.0-examples swig4.0-doc\n","The following NEW packages will be installed:\n","  swig swig4.0\n","0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 1,116 kB of archives.\n","After this operation, 5,542 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n","Fetched 1,116 kB in 1s (806 kB/s)\n","Selecting previously unselected package swig4.0.\n","(Reading database ... 121918 files and directories currently installed.)\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n","Collecting swig==4.* (from gymnasium[box2d])\n","  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349145 sha256=575d9cace6cd563b254f64ae7d538b369034d1dd34b550cc5bcfad0e6731ab1a\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: swig, box2d-py\n","Successfully installed box2d-py-2.3.5 swig-4.2.1\n"]}],"source":["!pip install gymnasium\n","!pip install \"gymnasium[atari, accept-rom-license]\"\n","!apt-get install -y swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mZaKXP_aMl9O","executionInfo":{"status":"ok","timestamp":1716619835031,"user_tz":-120,"elapsed":7237,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","from torch.autograd import Variable\n","from collections import deque, namedtuple\n","import gymnasium as gym\n","import glob\n","import io\n","import base64\n","import imageio\n","from IPython.display import HTML, display\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder"]},{"cell_type":"markdown","metadata":{"id":"EzlDKXvkXzGI"},"source":["### Construimos la red neuronal"]},{"cell_type":"markdown","metadata":{"id":"UtG6Zc83YYy3"},"source":["#### Feed Neural Network"]},{"cell_type":"markdown","source":["La definición de la red neuronal en este proyecto se realiza mediante la clase Network, que hereda de nn.Module de PyTorch. Aquí tienes una explicación detallada de la arquitectura de la red neuronal:"],"metadata":{"id":"w83cCgmhqFFA"}},{"cell_type":"code","source":["class Network(nn.Module):\n","\n","  def __init__(self, state_size, action_size, seed = 21):\n","    super(Network, self).__init__()\n","    self.seed = torch.manual_seed(seed)\n","    self.fc1 = nn.Linear(state_size, 64)\n","    self.fc2 = nn.Linear(64, 64)\n","    self.fc3 = nn.Linear(64, action_size)\n","\n","  def forward(self, state):\n","    x = self.fc1(state)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    return self.fc3(x)"],"metadata":{"id":"5LC9_yavP1B_","executionInfo":{"status":"ok","timestamp":1716620591940,"user_tz":-120,"elapsed":334,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Explicación de la arquitectura:\n","\n","- Capa de entrada (self.fc1): Esta es la primera capa totalmente conectada que toma el estado de entrada del entorno como entrada. La cantidad de neuronas en esta capa está determinada por state_size, que representa la dimensión del espacio de observación del entorno. En este caso, state_size es el número de variables de estado del entorno Lunar Lander.\n","\n","- Capas ocultas (self.fc2): La red tiene dos capas ocultas totalmente conectadas. Cada una de estas capas tiene 64 neuronas. Después de pasar por cada capa oculta, se aplica una función de activación ReLU (Rectified Linear Unit) para introducir no linealidad en la red y ayudar a aprender representaciones más complejas de los datos.\n","\n","- Capa de salida (self.fc3): Esta es la capa de salida de la red, que tiene tantas neuronas como acciones posibles en el entorno. En este caso, action_size representa el número de acciones discretas que el agente puede tomar en el entorno Lunar Lander. La red produce una salida para cada posible acción, representando la estimación de Q-valor para cada acción.\n","\n","La red neuronal consiste en una secuencia de tres capas totalmente conectadas, donde las dos primeras capas son capas ocultas con 64 neuronas cada una, y la tercera capa es la capa de salida que produce una estimación de Q para cada acción posible en el entorno."],"metadata":{"id":"8gPsdD_zqKUD"}},{"cell_type":"markdown","metadata":{"id":"T364fz9qZb2j"},"source":["### Configuración del entorno Lunar Lander"]},{"cell_type":"markdown","source":["En esta parte del código, se inicializa el entorno del Lunar Lander utilizando la biblioteca Gym de OpenAI."],"metadata":{"id":"_JH5W30yqgq1"}},{"cell_type":"code","source":["env = gym.make('LunarLander-v2')\n","state_shape = env.observation_space.shape\n","state_size = env.observation_space.shape[0]\n","number_actions = env.action_space.n\n","print('State shape: ', state_shape)\n","print('State size: ', state_size)\n","print('Number of actions: ', number_actions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZvPWTsVSbgM","outputId":"abf4ad4a-13ba-4515-b519-20423e5b8908","executionInfo":{"status":"ok","timestamp":1716620594125,"user_tz":-120,"elapsed":236,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["State shape:  (8,)\n","State size:  8\n","Number of actions:  4\n"]}]},{"cell_type":"markdown","source":["Importamos la biblioteca Gym, que proporciona una variedad de entornos para experimentar con algoritmos de aprendizaje por refuerzo. Creamos una instancia del entorno Lunar Lander utilizando la función make() de Gym. 'LunarLander-v2' es el identificador del entorno, que indica que queremos usar la versión 2 del entorno Lunar Lander. Esta versión del entorno es una versión actualizada y mejorada del entorno original.\n","Lunar Lander es un entorno clásico de aprendizaje por refuerzo donde el objetivo del agente es aterrizar un módulo lunar de forma segura en la superficie lunar, evitando colisiones y consumiendo la menor cantidad de combustible posible.\n","\n","Obtenemos la forma del espacio de observación del entorno Lunar Lander (state shape), que describe la forma de los estados que el agente recibe como entrada. En este caso, el espacio de observación consiste en un vector con 8 dimensiones.\n","\n","Obtenemops el tamaño del espacio de observación del entorno Lunar Lander, que es igual a la dimensión del espacio de observación. En este caso, state_size representa la cantidad de variables de estado del entorno. El estado del entorno se define por un vector de 8 dimensiones que contiene información sobre la posición, velocidad y orientación del módulo lunar, así como información sobre las piernas del módulo lunar (si están en contacto con la superficie lunar o no). Esta información permite al agente tomar decisiones informadas sobre cómo controlar el módulo lunar.\n","\n","Obtenemos el número de acciones posibles que el agente puede tomar en el entorno Lunar Lander con la función 'env.action_space.n'. Esta devuelve la cantidad de acciones en el espacio de acciones del entorno. El agente puede elegir entre 4 acciones discretas en cada paso de tiempo:\n","\n","- No hacer nada\n","- Encender el motor principal\n","- Encender el motor izquierdo\n","- Encender el motor derecho\n","\n","\n"],"metadata":{"id":"EmK6W-U8qh9m"}},{"cell_type":"markdown","metadata":{"id":"c_dZmOIvZgj-"},"source":["### Inicialización Hiperparámetros"]},{"cell_type":"markdown","source":["En esta parte del notebook decidimos definir los hiperparmétros"],"metadata":{"id":"4pERdMjAtXrz"}},{"cell_type":"code","source":["learning_rate = 5e-4\n","minibatch_size = 100\n","discount_factor = 0.99\n","replay_buffer_size = int(1e5)\n","interpolation_parameter = 1e-3"],"metadata":{"id":"s8Vg5hgOTbnr","executionInfo":{"status":"ok","timestamp":1716620595919,"user_tz":-120,"elapsed":344,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["- Tasa de aprendizaje (learning_rate): La tasa de aprendizaje controla la magnitud de los ajustes que se realizan a los pesos de la red neuronal durante el entrenamiento.\n","\n","- Tamaño del minibatch (minibatch_size): El tamaño del minibatch determina cuántas muestras de experiencia se utilizan en cada paso de entrenamiento de la red neuronal.\n","\n","- Factor de descuento (discount_factor): El factor de descuento controla la importancia relativa de las recompensas futuras en comparación con las recompensas inmediatas. Un factor de descuento cercano a 1 indica que el agente valora altamente las recompensas futuras, lo que fomenta la planificación a largo plazo. Un factor de descuento cercano a 0 indica que el agente valora menos las recompensas futuras.\n","- Tamaño del búfer de repetición (replay_buffer_size): El tamaño del búfer de repetición determina cuántas experiencias pasadas se almacenarán para el entrenamiento de la red neuronal. El búfer de repetición se utiliza en el método de memoria de repetición para almacenar y muestrear experiencias pasadas de manera eficiente.\n","\n","- Parámetro de interpolación (interpolation_parameter): El parámetro de interpolación se utiliza en el proceso de actualización suave de los pesos de la red neuronal objetivo. La actualización suave es una técnica que suaviza los cambios en los pesos de la red objetivo al combinar gradualmente los pesos de la red local y los pesos de la red objetivo. El parámetro de interpolación controla la proporción de contribución de los pesos de la red local y los pesos de la red objetivo en la actualización suave.\n"],"metadata":{"id":"NsuXA3oUteOD"}},{"cell_type":"markdown","metadata":{"id":"8hD_Vs-bYnip"},"source":["### Implementación Experience Replay"]},{"cell_type":"markdown","source":["En esta parte del código definimos la memoria de repetición, que es una técnica importante en el aprendizaje por refuerzo."],"metadata":{"id":"0IhIjdneuOy7"}},{"cell_type":"code","source":["class ReplayMemory(object):\n","\n","  def __init__(self, capacidad):\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    self.capacidad = capacidad\n","    self.memoria = []\n","\n","  def push(self, event):\n","    self.memoria.append(event)\n","    if len(self.memoria) > self.capacidad:\n","      del self.memoria[0]\n","\n","  def sample(self, batch_size):\n","    experiences = random.sample(self.memoria, k = batch_size)\n","    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n","    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n","    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n","    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n","    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n","    return states, next_states, actions, rewards, dones"],"metadata":{"id":"ESeB2WkTjv96","executionInfo":{"status":"ok","timestamp":1716620597503,"user_tz":-120,"elapsed":236,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["El constructor inicializa la memoria de repetición con una capacidad máxima especificada y el número máximo de experiencias pasadas que la memoria puede almacenar.\n","\n","Agregamos el método push para añadir una nueva experiencia al búfer de repetición.En el que creamos la tupla event que contiene la experiencia del agente.\n","Si la memoria ya ha alcanzado su capacidad máxima, se elimina la experiencia más antigua para dejar espacio para la nueva experiencia.\n","\n","También definimos el  método sample para realizar un muestreo aleatorio de un lote de experiencias del búfer de repetición.\n","batch_size especifica el tamaño del lote, es decir, cuántas experiencias se deben muestrear.\n","Se utilizan las experiencias muestreadas para construir tensores de estados, acciones, recompensas, siguientes estados y señales de terminación, que se utilizan para el entrenamiento de la red neuronal."],"metadata":{"id":"EzSPvmI0ualO"}},{"cell_type":"markdown","metadata":{"id":"JmEkbFbUY6Jt"},"source":["### Implementación del Agente"]},{"cell_type":"markdown","source":["#### DQN"],"metadata":{"id":"cTAyhErx3VeQ"}},{"cell_type":"markdown","source":["Definimos el agente DQN:"],"metadata":{"id":"dYuSCjLC3kRM"}},{"cell_type":"code","source":["class DQN():\n","\n","  def __init__(self, state_size, action_size):\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n","    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n","    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n","    self.memoria = ReplayMemory(replay_buffer_size)\n","    self.t_step = 0\n","\n","  def step(self, state, action, reward, next_state, done):\n","    self.memoria.push((state, action, reward, next_state, done))\n","    self.t_step = (self.t_step + 1) % 4\n","    if self.t_step == 0:\n","      if len(self.memoria.memoria) > minibatch_size:\n","        experiences = self.memoria.sample(100)\n","        self.learn(experiences, discount_factor)\n","\n","  def act(self, state, epsilon = 0.):\n","    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","    self.local_qnetwork.eval()\n","    with torch.no_grad():\n","      action_values = self.local_qnetwork(state)\n","    self.local_qnetwork.train()\n","    if random.random() > epsilon:\n","      return np.argmax(action_values.cpu().data.numpy())\n","    else:\n","      return random.choice(np.arange(self.action_size))\n","\n","  def learn(self, experiences, discount_factor):\n","    states, next_states, actions, rewards, dones = experiences\n","    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n","    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n","    q_expected = self.local_qnetwork(states).gather(1, actions)\n","    loss = F.mse_loss(q_expected, q_targets)\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()\n","    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n","\n","  def soft_update(self, local_model, target_model, interpolation_parameter):\n","    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"],"metadata":{"id":"6sj63JJ-tOcI","executionInfo":{"status":"ok","timestamp":1716620600210,"user_tz":-120,"elapsed":283,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Esta clase  define el agente que interactúa con el entorno Lunar Lander utilizando un método de aprendizaje por refuerzo conocido como DQN (Deep Q-Network).\n","\n","Inicializamos el agente por medio de una clase. En ella usamos el tamaño del espacio de estado y el tamaño del espacio de acción definidos previamente.\n","Utilizamos un par de instancias de la clase Network que representan las redes neuronales que aproximan la función Q. Una red neuronal representa el Q-valor para cada par estado-acción.\n","También nombramos un optimizador Adam que se utiliza para actualizar los pesos de la red neuronal local durante el entrenamiento y una instancia de ReplayMemory que almacena las experiencias pasadas del agente para su posterior uso en el entrenamiento.\n","Por último, definimos un contador utilizado para controlar cuándo se actualizan los pesos de la red neuronal objetivo.\n","\n","Tras esto, definimos el método 'step' que se llama cada vez que el agente realiza una acción en el entorno.\n","Agrega la experiencia actual (state, action, reward, next_state, done) a la memoria de repetición.\n","Incrementa self.t_step y, si self.t_step es un múltiplo de 4, se realiza un paso de aprendizaje llamando al método learn().\n","act(self, state, epsilon=0.):\n","\n","Este método elige una acción para el agente dado un estado.\n","Si epsilon es mayor que cero, se elige una acción de manera epsilon-greedy, lo que significa que el agente elige una acción aleatoria con probabilidad epsilon y la mejor acción según la red neuronal local con probabilidad 1 - epsilon.\n","Si epsilon es cero, el agente siempre elige la mejor acción según la red neuronal local.\n","learn(self, experiences, discount_factor):\n","\n","Este método realiza un paso de aprendizaje utilizando una muestra de experiencias pasadas de la memoria de repetición.\n","Calcula los objetivos Q utilizando la red neuronal objetivo y la función de pérdida de error cuadrático medio (MSE) entre los valores Q predichos por la red neuronal local y los objetivos Q calculados.\n","Actualiza los pesos de la red neuronal local utilizando el optimizador Adam y realiza una actualización suave de los pesos de la red neuronal objetivo.\n","soft_update(self, local_model, target_model, interpolation_parameter):\n","\n","Este método actualiza suavemente los pesos de la red neuronal objetivo utilizando una interpolación entre los pesos de la red neuronal local y los pesos de la red neuronal objetivo actuales.\n","Esto ayuda a estabilizar el entrenamiento y mejorar el rendimiento del agente."],"metadata":{"id":"4PAcTix83gb9"}},{"cell_type":"markdown","metadata":{"id":"o1tZElccZmf6"},"source":["Instanciamos el agente DQN, utilizando el tamaño del espacio de estado y el número de acciones disponibles definidas previamente."]},{"cell_type":"code","source":["agent = DQN(state_size, number_actions)"],"metadata":{"id":"UwzttV9c42y-","executionInfo":{"status":"ok","timestamp":1716620603669,"user_tz":-120,"elapsed":236,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8v0PtUfaVQp"},"source":["### Entrenamiento del agente"]},{"cell_type":"markdown","source":["En este chunk podemos observar los parámetros necesarios para proceder con el entrenamiento del agente:\n","\n","- `number_episodes`: Número total de episodios de entrenamiento que se ejecutarán.\n","- `maximum_number_timesteps_per_episode`: Número máximo de pasos de tiempo permitidos por episodio.\n","- `epsilon_starting_value`: Valor inicial de epsilon para la estrategia epsilon-greedy.\n","- `epsilon_ending_value`: Valor final de epsilon después del decaimiento.\n","- `epsilon_decay_value`: Factor de decaimiento para epsilon en cada episodio.\n","- `epsilon`: Inicialización de epsilon con su valor inicial.\n","- `scores_on_100_episodes`: Una cola de tamaño fijo que almacena las puntuaciones de los últimos 100 episodios para calcular el promedio móvil de la puntuación."],"metadata":{"id":"sP1RTbes7hV5"}},{"cell_type":"code","source":["number_episodes = 2000\n","maximum_number_timesteps_per_episode = 1000\n","epsilon_starting_value  = 1.0\n","epsilon_ending_value  = 0.01\n","epsilon_decay_value  = 0.995\n","epsilon = epsilon_starting_value\n","scores_on_100_episodes = deque(maxlen = 100)\n"],"metadata":{"id":"_VRlWau859gZ","executionInfo":{"status":"ok","timestamp":1716620605262,"user_tz":-120,"elapsed":220,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Y ahora procedemos a detallar el proceso de entrenamiento del agente:\n","\n","Se ejecuta un bucle que itera sobre cada episodio de entrenamiento.\n","En cada episodio, se reinicia el entorno (env.reset()) para obtener el estado inicial. Luego, se ejecuta un bucle interno que itera sobre cada paso de tiempo en el episodio actual.\n","En cada paso de tiempo, el agente selecciona una acción utilizando su política actual, que puede ser epsilon-greedy (agent.act()).\n","La acción se ejecuta en el entorno (env.step(action)) y se obtiene el siguiente estado, la recompensa, y si el episodio ha terminado.\n","El agente almacena la experiencia actual en su memoria de repetición (agent.step()).\n","Se actualiza el estado actual con el siguiente estado y se suma la recompensa al puntaje acumulado del episodio.\n","Si el episodio ha terminado (done == True), el ciclo interno se detiene.\n","Se agrega el puntaje del episodio a la cola de puntuaciones de los últimos 100 episodios.\n","Se actualiza el valor de epsilon utilizando el decaimiento programado.\n","Se imprime el número de episodio y el promedio móvil de la puntuación de los últimos 100 episodios.\n","Si el promedio móvil de la puntuación de los últimos 100 episodios alcanza 200 o más, se imprime un mensaje indicando que el entorno se ha resuelto y se guarda el modelo del agente.\n","El ciclo de entrenamiento se detiene si se resuelve el entorno o si se alcanza el número máximo de episodios de entrenamiento."],"metadata":{"id":"fuQvdimm7fck"}},{"cell_type":"code","source":["\n","for episode in range(1, number_episodes + 1):\n","  state, _ = env.reset()\n","  score = 0\n","  for t in range(maximum_number_timesteps_per_episode):\n","    action = agent.act(state, epsilon)\n","    next_state, reward, done, _, _ = env.step(action)\n","    agent.step(state, action, reward, next_state, done)\n","    state = next_state\n","    score += reward\n","    if done:\n","      break\n","  scores_on_100_episodes.append(score)\n","  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n","  print('\\rEpisodio {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n","  if episode % 100 == 0:\n","    print('\\rEpisodio {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n","  if np.mean(scores_on_100_episodes) >= 200.0:\n","    print('\\nEnvironment resuelto en {:d} episodios!\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n","    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n","    break"],"metadata":{"id":"Xmo-m9fL7Srm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716621003994,"user_tz":-120,"elapsed":397037,"user":{"displayName":"Vicent","userId":"09438679983193344385"}},"outputId":"12851bf1-35e9-491b-a9ef-85791626463a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Episodio 100\tAverage Score: -161.58\n","Episodio 200\tAverage Score: -81.91\n","Episodio 300\tAverage Score: -29.94\n","Episodio 400\tAverage Score: 23.15\n","Episodio 500\tAverage Score: 194.54\n","Episodio 510\tAverage Score: 200.38\n","Environment resuelto en 510 episodios!\tAverage Score: 200.38\n"]}]},{"cell_type":"markdown","metadata":{"id":"O8CNwdOTcCoP"},"source":["### Visualización los resultados"]},{"cell_type":"markdown","source":["Estas funciones se utilizan para visualizar un video del modelo entrenado en el entorno Lunar Lander v2. La primera función crea y guarda el video, mientras que la segunda función muestra el video si está disponible."],"metadata":{"id":"uj3a3V9Z8Zxv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb9nVvU2Okhk","executionInfo":{"status":"aborted","timestamp":1716620560136,"user_tz":-120,"elapsed":5,"user":{"displayName":"Vicent","userId":"09438679983193344385"}}},"outputs":[],"source":["def show_video_of_model(agent, env_name):\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    state, _ = env.reset()\n","    done = False\n","    frames = []\n","    while not done:\n","        frame = env.render()\n","        frames.append(frame)\n","        action = agent.act(state)\n","        state, reward, done, _, _ = env.step(action.item())\n","    env.close()\n","    imageio.mimsave('video.mp4', frames, fps=30)\n","\n","show_video_of_model(agent, 'LunarLander-v2')\n","\n","def show_video():\n","    mp4list = glob.glob('*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"No hay video\")\n","\n","show_video()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}